import torch
import torch.nn as nn
from tqdm import tqdm

def fine_tune_projection_layer(
    dataloader,
    vit_model,
    projector,
    phi_model,
    phi_tokenizer,
    device,
    num_epochs=3,
    lr=1e-4,
    save_path="projector_finetuned.safetensors"
):
    # Freeze ViT and Phi-2
    vit_model.eval()
    phi_model.eval()
    for p in vit_model.parameters():
        p.requires_grad = False
    for p in phi_model.parameters():
        p.requires_grad = False

    # Enable training on projector
    projector.train()
    optimizer = torch.optim.AdamW(projector.parameters(), lr=lr)
    loss_fn = nn.CrossEntropyLoss(ignore_index=phi_tokenizer.pad_token_id)

    for epoch in range(num_epochs):
        epoch_loss = 0
        for batch in tqdm(dataloader, desc=f"Epoch {epoch+1}"):
            images = batch["image"].to(device)           # [B, 3, H, W]
            questions = batch["question"]                # List[str]
            answers = batch["answer"]                    # List[str]

            # 1. Get CLS token from ViT
            with torch.no_grad():
                vit_out = vit_model(pixel_values=images)
                cls_token = vit_out.last_hidden_state[:, 0, :]  # [B, vit_dim]
            projected_embed = projector(cls_token).unsqueeze(1).to(torch.float16)  # [B, 1, phi_dim]

            # 2. Tokenize question + answer combined prompt
            prompts = [f"<image> Question: {q} Answer:" for q in questions]
            input_encodings = phi_tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(device)
            label_encodings = phi_tokenizer(answers, return_tensors="pt", padding=True, truncation=True).to(device)

            # 3. Token embeddings
            token_embeds = phi_model.get_input_embeddings()(input_encodings.input_ids).to(torch.float16)  # [B, T, D]
            inputs_embeds = torch.cat([projected_embed, token_embeds], dim=1)  # [B, T+1, D]

            # 4. Attention mask
            attention_mask = torch.cat([
                torch.ones((token_embeds.size(0), 1), dtype=torch.long).to(device),
                input_encodings.attention_mask
            ], dim=1)

            # 5. Labels (mask input part with -100)
            labels = label_encodings.input_ids
            masked_labels = torch.full((labels.shape[0], input_encodings.input_ids.shape[1] + 1), -100).to(device)
            masked_labels[:, -labels.shape[1]:] = labels  # only keep answer part

            # 6. Forward
            output = phi_model(
                inputs_embeds=inputs_embeds,
                attention_mask=attention_mask,
                labels=masked_labels
            )

            loss = output.loss
            epoch_loss += loss.item()

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        print(f"Epoch {epoch+1} Loss: {epoch_loss / len(dataloader):.4f}")

    # Save trained projector
    from safetensors.torch import save_file
    save_file(projector.state_dict(), save_path)
    print(f"Projection layer saved to: {save_path}")

######  Example usage
fine_tune_projection_layer(
    dataloader=new_dataset_dataloader,
    vit_model=vit_model,
    projector=projector,
    phi_model=phi_model,
    phi_tokenizer=phi_tokenizer,
    device=device,
    num_epochs=5,
    lr=5e-5
)
