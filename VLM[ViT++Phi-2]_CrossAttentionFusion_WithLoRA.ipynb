{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Attention for feature combination [Visual and Text Features]\\\n",
    "To replace simple concatenation of projected_embed and token embeddings with cross-attention between the image features and the text features, There is a need to introduce a cross-attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import ViTModel, AutoConfig\n",
    "from safetensors.torch import load_file\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load ViT (Frozen) ---\n",
    "vit_config = AutoConfig.from_pretrained(\"./vit\")\n",
    "vit_model = ViTModel.from_pretrained(\"./vit\", config=vit_config)\n",
    "vit_model.load_state_dict(load_file(\"./vit/model.safetensors\"))\n",
    "vit_model = vit_model.to(device).eval()\n",
    "for p in vit_model.parameters(): p.requires_grad = False\n",
    "\n",
    "# --- Load Phi-2 (Frozen) ---\n",
    "phi_tokenizer = AutoTokenizer.from_pretrained(\"./phi-2\")\n",
    "phi_tokenizer.pad_token = phi_tokenizer.eos_token\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./phi-2\", torch_dtype=torch.float16, device_map={\"\": device}\n",
    ")\n",
    "phi_model.eval()\n",
    "for p in phi_model.parameters(): p.requires_grad = False\n",
    "\n",
    "# --- Dimensions ---\n",
    "vit_dim = vit_model.config.hidden_size\n",
    "phi_dim = phi_model.config.hidden_size\n",
    "\n",
    "# --- Projection Layer (Trainable) ---\n",
    "projector = nn.Linear(vit_dim, phi_dim).to(device)\n",
    "\n",
    "# --- Cross-Attention Module (Trainable) ---\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, text_embeds, image_embed):\n",
    "        # image_embed: [B, 1, D] (key/value), text_embeds: [B, T, D] (query)\n",
    "        attended, _ = self.attn(query=text_embeds, key=image_embed, value=image_embed)\n",
    "        return self.ln(text_embeds + attended)  # residual connection\n",
    "\n",
    "cross_attn = CrossAttention(phi_dim).to(device)\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = torch.optim.AdamW(list(projector.parameters()) + list(cross_attn.parameters()), lr=1e-4)\n",
    "\n",
    "# --- Loss ---\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=phi_tokenizer.pad_token_id)\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_projection_layer(dataloader, projector, epochs=3):\n",
    "    projector.train()\n",
    "    cross_attn.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            questions = batch[\"question\"]\n",
    "            answers = batch[\"answer\"]\n",
    "\n",
    "            # Visual features\n",
    "            with torch.no_grad():\n",
    "                vit_out = vit_model(pixel_values=images)\n",
    "                cls_token = vit_out.last_hidden_state[:, 0, :]  # [B, vit_dim]\n",
    "            projected_image = projector(cls_token).unsqueeze(1).to(torch.float16)  # [B, 1, phi_dim]\n",
    "\n",
    "            # Textual embeddings\n",
    "            inputs = phi_tokenizer(\n",
    "                [q + \" \" + phi_tokenizer.eos_token for q in questions],\n",
    "                return_tensors=\"pt\", padding=True, truncation=True\n",
    "            ).to(device)\n",
    "\n",
    "            labels = phi_tokenizer(\n",
    "                answers, return_tensors=\"pt\", padding=True, truncation=True\n",
    "            ).input_ids.to(device)\n",
    "\n",
    "            token_embeds = phi_model.model.embed_tokens(inputs.input_ids).to(torch.float16)  # [B, T, D]\n",
    "\n",
    "            # Cross-attention: text attends to image\n",
    "            fused_embeds = cross_attn(token_embeds, projected_image)  # [B, T, D]\n",
    "\n",
    "            attention_mask = inputs.attention_mask\n",
    "            labels = labels\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = phi_model(\n",
    "                inputs_embeds=fused_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code with some modification in the loss function calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code with some modification in the loss function calculation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import ViTModel, AutoConfig\n",
    "from safetensors.torch import load_file\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load ViT (Frozen) ---\n",
    "vit_config = AutoConfig.from_pretrained(\"./vit\")\n",
    "vit_model = ViTModel.from_pretrained(\"./vit\", config=vit_config)\n",
    "vit_model.load_state_dict(load_file(\"./vit/model.safetensors\"))\n",
    "vit_model = vit_model.to(device).eval()\n",
    "for p in vit_model.parameters(): p.requires_grad = False\n",
    "\n",
    "# --- Load Phi-2 (Frozen) ---\n",
    "phi_tokenizer = AutoTokenizer.from_pretrained(\"./phi-2\")\n",
    "phi_tokenizer.pad_token = phi_tokenizer.eos_token\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./phi-2\", torch_dtype=torch.float16, device_map={\"\": device}\n",
    ")\n",
    "phi_model.eval()\n",
    "for p in phi_model.parameters(): p.requires_grad = False\n",
    "\n",
    "# --- Dimensions ---\n",
    "vit_dim = vit_model.config.hidden_size\n",
    "phi_dim = phi_model.config.hidden_size\n",
    "\n",
    "# --- Projection Layer (Trainable) ---\n",
    "projector = nn.Linear(vit_dim, phi_dim).to(device)\n",
    "\n",
    "# --- Cross-Attention Module (Trainable) ---\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, text_embeds, image_embed):\n",
    "        # image_embed: [B, 1, D] (key/value), text_embeds: [B, T, D] (query)\n",
    "        attended, _ = self.attn(query=text_embeds, key=image_embed, value=image_embed)\n",
    "        return self.ln(text_embeds + attended)\n",
    "\n",
    "cross_attn = CrossAttention(phi_dim).to(device)\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = torch.optim.AdamW(list(projector.parameters()) + list(cross_attn.parameters()), lr=1e-4)\n",
    "\n",
    "# --- Explicit CrossEntropy Loss ---\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=phi_tokenizer.pad_token_id)\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_projection_layer(dataloader, projector, epochs=3):\n",
    "    projector.train()\n",
    "    cross_attn.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            questions = batch[\"question\"]\n",
    "            answers = batch[\"answer\"]\n",
    "\n",
    "            # ViT image features\n",
    "            with torch.no_grad():\n",
    "                vit_out = vit_model(pixel_values=images)\n",
    "                cls_token = vit_out.last_hidden_state[:, 0, :]\n",
    "            projected_image = projector(cls_token).unsqueeze(1).to(torch.float16)  # [B, 1, D]\n",
    "\n",
    "            # Tokenize prompt + answer\n",
    "            prompts = [f\"Question: {q.strip()} Answer:\" for q in questions]\n",
    "            full_texts = [f\"{p} {a.strip()}\" for p, a in zip(prompts, answers)]\n",
    "\n",
    "            inputs = phi_tokenizer(full_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            input_ids = inputs.input_ids.to(device)\n",
    "            attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "            # Generate label masks to ignore prompt\n",
    "            with phi_tokenizer.as_target_tokenizer():\n",
    "                prompt_ids = phi_tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "            labels = input_ids.clone()\n",
    "            for i in range(labels.size(0)):\n",
    "                prompt_len = (prompt_ids[i] != phi_tokenizer.pad_token_id).sum()\n",
    "                labels[i, :prompt_len] = -100\n",
    "\n",
    "            # Token embedding + cross-attention fusion\n",
    "            token_embeds = phi_model.get_input_embeddings()(input_ids).to(torch.float16)  # [B, T, D]\n",
    "            fused_embeds = cross_attn(token_embeds, projected_image)  # [B, T, D]\n",
    "\n",
    "            # Forward pass to get logits\n",
    "            logits = phi_model(inputs_embeds=fused_embeds, attention_mask=attention_mask).logits  # [B, T, V]\n",
    "\n",
    "            # Compute loss manually\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code with validation loop and ANLS metric\n",
    "1. Validation loop\n",
    "\n",
    "2. ANLS (Average Normalized Levenshtein Similarity) metric\n",
    "\n",
    "3. Manual CrossEntropyLoss\n",
    "\n",
    "4. Cross-attention fusion between ViT and Phi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Code with validation loop and ANLS metric\n",
    "import numpy as np\n",
    "import editdistance  # pip install editdistance\n",
    "\n",
    "def normalized_levenshtein(pred, gt):\n",
    "    pred, gt = pred.strip().lower(), gt.strip().lower()\n",
    "    if len(gt) == 0:\n",
    "        return 1.0 if len(pred) == 0 else 0.0\n",
    "    dist = editdistance.eval(pred, gt)\n",
    "    norm = dist / max(len(pred), len(gt))\n",
    "    return 1 - norm\n",
    "\n",
    "def compute_anls(preds, gts, threshold=0.5):\n",
    "    scores = []\n",
    "    for p, g in zip(preds, gts):\n",
    "        sim = normalized_levenshtein(p, g)\n",
    "        scores.append(sim if sim >= threshold else 0)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_projection_layer_with_validation(train_loader, val_loader, projector, epochs=3):\n",
    "    projector.train()\n",
    "    cross_attn.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_anls = []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}\"):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            questions = batch[\"question\"]\n",
    "            answers = batch[\"answer\"]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                vit_out = vit_model(pixel_values=images)\n",
    "                cls_token = vit_out.last_hidden_state[:, 0, :]\n",
    "            projected_image = projector(cls_token).unsqueeze(1).to(torch.float16)\n",
    "\n",
    "            prompts = [f\"Question: {q.strip()} Answer:\" for q in questions]\n",
    "            full_texts = [f\"{p} {a.strip()}\" for p, a in zip(prompts, answers)]\n",
    "\n",
    "            inputs = phi_tokenizer(full_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            input_ids = inputs.input_ids\n",
    "            attention_mask = inputs.attention_mask\n",
    "\n",
    "            with phi_tokenizer.as_target_tokenizer():\n",
    "                prompt_ids = phi_tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "            labels = input_ids.clone()\n",
    "            for i in range(labels.size(0)):\n",
    "                prompt_len = (prompt_ids[i] != phi_tokenizer.pad_token_id).sum()\n",
    "                labels[i, :prompt_len] = -100\n",
    "\n",
    "            token_embeds = phi_model.get_input_embeddings()(input_ids).to(torch.float16)\n",
    "            fused_embeds = cross_attn(token_embeds, projected_image)\n",
    "\n",
    "            logits = phi_model(inputs_embeds=fused_embeds, attention_mask=attention_mask).logits\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # --- In-batch ANLS ---\n",
    "            with torch.no_grad():\n",
    "                generated_ids = phi_model.generate(\n",
    "                    inputs_embeds=fused_embeds,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                decoded_preds = phi_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "                cleaned_preds = [pred.replace(prompt, \"\").strip() for pred, prompt in zip(decoded_preds, prompts)]\n",
    "                anls_score = compute_anls(cleaned_preds, answers)\n",
    "                total_anls.append(anls_score)\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_anls = np.mean(total_anls)\n",
    "        print(f\"[Train] Epoch {epoch+1} | Loss: {avg_loss:.4f} | ANLS: {avg_anls:.4f}\")\n",
    "\n",
    "        # Run Validation\n",
    "        run_validation(val_loader, projector)\n",
    "\n",
    "def run_validation(val_loader, projector):\n",
    "    projector.eval()\n",
    "    cross_attn.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"[Val]\"):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            questions = batch[\"question\"]\n",
    "            answers = batch[\"answer\"]\n",
    "\n",
    "            vit_out = vit_model(pixel_values=images)\n",
    "            cls_token = vit_out.last_hidden_state[:, 0, :]\n",
    "            projected_image = projector(cls_token).unsqueeze(1).to(torch.float16)\n",
    "\n",
    "            prompts = [f\"Question: {q.strip()} Answer:\" for q in questions]\n",
    "            full_texts = [f\"{p} {a.strip()}\" for p, a in zip(prompts, answers)]\n",
    "\n",
    "            inputs = phi_tokenizer(full_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            input_ids = inputs.input_ids\n",
    "            attention_mask = inputs.attention_mask\n",
    "\n",
    "            with phi_tokenizer.as_target_tokenizer():\n",
    "                prompt_ids = phi_tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "            labels = input_ids.clone()\n",
    "            for i in range(labels.size(0)):\n",
    "                prompt_len = (prompt_ids[i] != phi_tokenizer.pad_token_id).sum()\n",
    "                labels[i, :prompt_len] = -100\n",
    "\n",
    "            token_embeds = phi_model.get_input_embeddings()(input_ids).to(torch.float16)\n",
    "            fused_embeds = cross_attn(token_embeds, projected_image)\n",
    "\n",
    "            logits = phi_model(inputs_embeds=fused_embeds, attention_mask=attention_mask).logits\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            generated_ids = phi_model.generate(\n",
    "                inputs_embeds=fused_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=False\n",
    "            )\n",
    "            decoded_preds = phi_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            cleaned_preds = [pred.replace(prompt, \"\").strip() for pred, prompt in zip(decoded_preds, prompts)]\n",
    "            all_preds.extend(cleaned_preds)\n",
    "            all_labels.extend(answers)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    anls = compute_anls(all_preds, all_labels)\n",
    "    print(f\"[Val] Loss: {avg_loss:.4f} | ANLS: {anls:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_projection_layer_with_validation(\u001b[43mtrain_loader\u001b[49m, val_loader, projector, epochs=\u001b[32m3\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "train_projection_layer_with_validation(train_loader, val_loader, projector, epochs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including Model checkpoint saving on best ANLS and progress plots\n",
    "1. Model checkpointing based on the best validation ANLS\n",
    "\n",
    "2. Progress plots for training loss and validation ANLS\n",
    "\n",
    "3. Clean structure, preserving all previous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_projection_layer_with_validation_and_checkpoint(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    projector,\n",
    "    epochs=3,\n",
    "    checkpoint_dir=\"./checkpoints\"\n",
    "):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    projector.train()\n",
    "    cross_attn.train()\n",
    "\n",
    "    best_anls = -1\n",
    "    history = {\"train_loss\": [], \"val_anls\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_anls = []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}\"):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            questions = batch[\"question\"]\n",
    "            answers = batch[\"answer\"]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                vit_out = vit_model(pixel_values=images)\n",
    "                cls_token = vit_out.last_hidden_state[:, 0, :]\n",
    "            projected_image = projector(cls_token).unsqueeze(1).to(torch.float16)\n",
    "\n",
    "            prompts = [f\"Question: {q.strip()} Answer:\" for q in questions]\n",
    "            full_texts = [f\"{p} {a.strip()}\" for p, a in zip(prompts, answers)]\n",
    "\n",
    "            inputs = phi_tokenizer(full_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            input_ids = inputs.input_ids\n",
    "            attention_mask = inputs.attention_mask\n",
    "\n",
    "            with phi_tokenizer.as_target_tokenizer():\n",
    "                prompt_ids = phi_tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "            labels = input_ids.clone()\n",
    "            for i in range(labels.size(0)):\n",
    "                prompt_len = (prompt_ids[i] != phi_tokenizer.pad_token_id).sum()\n",
    "                labels[i, :prompt_len] = -100\n",
    "\n",
    "            token_embeds = phi_model.get_input_embeddings()(input_ids).to(torch.float16)\n",
    "            fused_embeds = cross_attn(token_embeds, projected_image)\n",
    "\n",
    "            logits = phi_model(inputs_embeds=fused_embeds, attention_mask=attention_mask).logits\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                generated_ids = phi_model.generate(\n",
    "                    inputs_embeds=fused_embeds,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                decoded_preds = phi_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "                cleaned_preds = [pred.replace(prompt, \"\").strip() for pred, prompt in zip(decoded_preds, prompts)]\n",
    "                anls_score = compute_anls(cleaned_preds, answers)\n",
    "                total_anls.append(anls_score)\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_anls = run_validation(val_loader, projector, save_best=True, best_anls=best_anls, checkpoint_dir=checkpoint_dir)\n",
    "\n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "        history[\"val_anls\"].append(avg_anls)\n",
    "\n",
    "        print(f\"[Train] Epoch {epoch+1} | Loss: {avg_loss:.4f} | Train ANLS: {np.mean(total_anls):.4f} | Val ANLS: {avg_anls:.4f}\")\n",
    "\n",
    "        if avg_anls > best_anls:\n",
    "            best_anls = avg_anls\n",
    "            print(f\"âœ… New best ANLS: {best_anls:.4f} â€” model saved.\")\n",
    "\n",
    "    # Plot loss and ANLS\n",
    "    plot_metrics(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(val_loader, projector, save_best=False, best_anls=-1, checkpoint_dir=\"./checkpoints\"):\n",
    "    projector.eval()\n",
    "    cross_attn.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"[Val]\"):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            questions = batch[\"question\"]\n",
    "            answers = batch[\"answer\"]\n",
    "\n",
    "            vit_out = vit_model(pixel_values=images)\n",
    "            cls_token = vit_out.last_hidden_state[:, 0, :]\n",
    "            projected_image = projector(cls_token).unsqueeze(1).to(torch.float16)\n",
    "\n",
    "            prompts = [f\"Question: {q.strip()} Answer:\" for q in questions]\n",
    "            full_texts = [f\"{p} {a.strip()}\" for p, a in zip(prompts, answers)]\n",
    "\n",
    "            inputs = phi_tokenizer(full_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            input_ids = inputs.input_ids\n",
    "            attention_mask = inputs.attention_mask\n",
    "\n",
    "            with phi_tokenizer.as_target_tokenizer():\n",
    "                prompt_ids = phi_tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "            labels = input_ids.clone()\n",
    "            for i in range(labels.size(0)):\n",
    "                prompt_len = (prompt_ids[i] != phi_tokenizer.pad_token_id).sum()\n",
    "                labels[i, :prompt_len] = -100\n",
    "\n",
    "            token_embeds = phi_model.get_input_embeddings()(input_ids).to(torch.float16)\n",
    "            fused_embeds = cross_attn(token_embeds, projected_image)\n",
    "\n",
    "            logits = phi_model(inputs_embeds=fused_embeds, attention_mask=attention_mask).logits\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            generated_ids = phi_model.generate(\n",
    "                inputs_embeds=fused_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=False\n",
    "            )\n",
    "            decoded_preds = phi_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            cleaned_preds = [pred.replace(prompt, \"\").strip() for pred, prompt in zip(decoded_preds, prompts)]\n",
    "            all_preds.extend(cleaned_preds)\n",
    "            all_labels.extend(answers)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    anls = compute_anls(all_preds, all_labels)\n",
    "\n",
    "    # Save checkpoint if best\n",
    "    if save_best and anls > best_anls:\n",
    "        from safetensors.torch import save_file\n",
    "        save_file(projector.state_dict(), f\"{checkpoint_dir}/projector_best_anls.safetensors\")\n",
    "        save_file(cross_attn.state_dict(), f\"{checkpoint_dir}/cross_attn_best_anls.safetensors\")\n",
    "\n",
    "    print(f\"[Val] Loss: {avg_loss:.4f} | ANLS: {anls:.4f}\")\n",
    "    return anls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plotting function\n",
    "\n",
    "def plot_metrics(history):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history[\"val_anls\"], label=\"Validation ANLS\", color=\"green\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"ANLS\")\n",
    "    plt.title(\"Validation ANLS\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To run the model\n",
    "train_projection_layer_with_validation_and_checkpoint(train_loader, val_loader, projector, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resume training from a checkpoint and save model in .safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####    Add helper functions to save and load checkpoint:\n",
    "\n",
    "from safetensors.torch import save_file, load_file\n",
    "\n",
    "def save_checkpoint(projector, cross_attn, checkpoint_dir=\"./checkpoints\"):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    save_file(projector.state_dict(), f\"{checkpoint_dir}/projector_best_anls.safetensors\")\n",
    "    save_file(cross_attn.state_dict(), f\"{checkpoint_dir}/cross_attn_best_anls.safetensors\")\n",
    "    print(f\"âœ… Checkpoint saved to {checkpoint_dir}\")\n",
    "\n",
    "def load_checkpoint(projector, cross_attn, checkpoint_dir=\"./checkpoints\"):\n",
    "    projector.load_state_dict(load_file(f\"{checkpoint_dir}/projector_best_anls.safetensors\"))\n",
    "    cross_attn.load_state_dict(load_file(f\"{checkpoint_dir}/cross_attn_best_anls.safetensors\"))\n",
    "    print(f\"ðŸ” Resumed from checkpoint in {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   Add resume Option to Training Function......Update training function definition like:\n",
    "\n",
    "def train_projection_layer_with_validation_and_checkpoint(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    projector,\n",
    "    epochs=3,\n",
    "    checkpoint_dir=\"./checkpoints\",\n",
    "    resume=False\n",
    "):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    if resume:\n",
    "        load_checkpoint(projector, cross_attn, checkpoint_dir)\n",
    "\n",
    "    projector.train()\n",
    "    cross_attn.train()\n",
    "    best_anls = -1\n",
    "    history = {\"train_loss\": [], \"val_anls\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######   Inside validation:\n",
    "\n",
    "# Save if best\n",
    "if save_best and anls > best_anls:\n",
    "    best_anls = anls\n",
    "    save_checkpoint(projector, cross_attn, checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   Run training with Resume\n",
    "\n",
    "train_projection_layer_with_validation_and_checkpoint(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    projector,\n",
    "    epochs=5,\n",
    "    checkpoint_dir=\"./checkpoints\",\n",
    "    resume=True  # <-- Resume from .safetensors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding LoRA layer on vision Features\\\n",
    "To integrate LoRA (Low-Rank Adaptation) on the vision features (i.e., the output of the ViT encoder), we can apply a LoRA adapter on the CLS token features before the projection layer.\n",
    "1. Add a LoRA module that adapts the CLS embedding ([B, D] from ViT).\n",
    "\n",
    "2. Apply LoRA before the linear projection to Phi-2 space.\n",
    "\n",
    "3. Train only the LoRA and projector (rest remains frozen).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install [LoRA] if needed\n",
    "#pip install peft\n",
    "# 2. Define LoRA Layer (low-rank adapter)\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, input_dim, rank=4, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.lora_down = nn.Linear(input_dim, rank, bias=False)\n",
    "        self.lora_up = nn.Linear(rank, input_dim, bias=False)\n",
    "        self.alpha = alpha\n",
    "        self.scale = alpha / rank\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.scale * self.lora_up(self.lora_down(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Modify Your Model Setup\n",
    "Replace below line: \\\n",
    "projector = nn.Linear(vit_dim, phi_dim).to(device)\\\n",
    "\n",
    "with this:\\\n",
    "lora_adapter = LoRALinear(vit_dim, rank=8, alpha=16).to(device)\\\n",
    "projector = nn.Linear(vit_dim, phi_dim).to(device)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Modify Training Code (Only This Part)\n",
    "#Replace this line:\n",
    "projected_image = projector(cls_token).unsqueeze(1).to(torch.float16)\n",
    "\n",
    "#With: \n",
    "cls_token = lora_adapter(cls_token)                       # Apply LoRA on ViT CLS\n",
    "projected_image = projector(cls_token).unsqueeze(1).to(torch.float16)\n",
    "\n",
    "\n",
    "# 5. Ensure LoRA is Trainable\n",
    "# Only lora_adapter and projector should be trained:\n",
    "for p in lora_adapter.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "for p in projector.parameters():\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply LoRA on the entire ViT layer\n",
    "\n",
    "To apply LoRA on the entire ViT layers, we can inject LoRA adapters into the attention modules of the ViT transformer blocks â€” typically in the query and value projection layers of MultiHeadAttention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    1. Define LoRA module \n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, in_features, r=4, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        self.lora_down = nn.Linear(in_features, r, bias=False)\n",
    "        self.lora_up = nn.Linear(r, in_features, bias=False)\n",
    "        nn.init.kaiming_uniform_(self.lora_down.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_up.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.scaling * self.lora_up(self.lora_down(x))\n",
    "\n",
    "#     2. Modify ViT Attention Layers (Inject LoRA)\n",
    "# This injects LoRA into the query and value projections of ViT.\n",
    "\n",
    "import types\n",
    "import math\n",
    "\n",
    "def inject_lora_into_vit_attention(vit_model, r=4, alpha=1.0):\n",
    "    for i, block in enumerate(vit_model.encoder.layer):\n",
    "        attn = block.attention.attention\n",
    "        d_model = attn.query.in_features\n",
    "\n",
    "        # Attach LoRA modules\n",
    "        attn.lora_q = LoRALinear(d_model, r, alpha).to(attn.query.weight.device)\n",
    "        attn.lora_v = LoRALinear(d_model, r, alpha).to(attn.value.weight.device)\n",
    "\n",
    "        # Patch forward function\n",
    "        original_forward = attn.forward\n",
    "\n",
    "        def lora_forward(self, hidden_states, head_mask=None, output_attentions=False):\n",
    "            # Standard projections\n",
    "            mixed_query_layer = self.query(hidden_states) + self.lora_q(hidden_states)\n",
    "            mixed_key_layer = self.key(hidden_states)\n",
    "            mixed_value_layer = self.value(hidden_states) + self.lora_v(hidden_states)\n",
    "\n",
    "            query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "            key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "            value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "            attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "            attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "            if head_mask is not None:\n",
    "                attention_scores = attention_scores + head_mask\n",
    "            attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "            context_layer = torch.matmul(attention_probs, value_layer)\n",
    "            context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "            new_context_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "            context_layer = context_layer.view(*new_context_shape)\n",
    "            return context_layer, attention_probs if output_attentions else context_layer\n",
    "\n",
    "        # Bind the new forward method\n",
    "        attn.forward = types.MethodType(lora_forward, attn)\n",
    "\n",
    "#    3. Apply to Loaded ViT Model\n",
    "# Call this immediately after loading ViT:\n",
    "inject_lora_into_vit_attention(vit_model, r=8, alpha=16)\n",
    "\n",
    "#    4. Enable LoRA Parameters for Training\n",
    "# Freeze all ViT weights\n",
    "for param in vit_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Enable LoRA adapters\n",
    "for name, module in vit_model.named_modules():\n",
    "    if isinstance(module, LoRALinear):\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "#   5.  Add LoRA Parameters to Optimizer\n",
    "lora_params = [p for n, p in vit_model.named_parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(projector.parameters()) + lora_params,\n",
    "    lr=1e-4\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project1_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
