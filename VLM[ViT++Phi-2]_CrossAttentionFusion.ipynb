{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Attention for feature combination [Visual and Text Features]\\\n",
    "To replace simple concatenation of projected_embed and token embeddings with cross-attention between the image features and the text features, There is a need to introduce a cross-attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import ViTModel, AutoConfig\n",
    "from safetensors.torch import load_file\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load ViT (Frozen) ---\n",
    "vit_config = AutoConfig.from_pretrained(\"./vit\")\n",
    "vit_model = ViTModel.from_pretrained(\"./vit\", config=vit_config)\n",
    "vit_model.load_state_dict(load_file(\"./vit/model.safetensors\"))\n",
    "vit_model = vit_model.to(device).eval()\n",
    "for p in vit_model.parameters(): p.requires_grad = False\n",
    "\n",
    "# --- Load Phi-2 (Frozen) ---\n",
    "phi_tokenizer = AutoTokenizer.from_pretrained(\"./phi-2\")\n",
    "phi_tokenizer.pad_token = phi_tokenizer.eos_token\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./phi-2\", torch_dtype=torch.float16, device_map={\"\": device}\n",
    ")\n",
    "phi_model.eval()\n",
    "for p in phi_model.parameters(): p.requires_grad = False\n",
    "\n",
    "# --- Dimensions ---\n",
    "vit_dim = vit_model.config.hidden_size\n",
    "phi_dim = phi_model.config.hidden_size\n",
    "\n",
    "# --- Projection Layer (Trainable) ---\n",
    "projector = nn.Linear(vit_dim, phi_dim).to(device)\n",
    "\n",
    "# --- Cross-Attention Module (Trainable) ---\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, text_embeds, image_embed):\n",
    "        # image_embed: [B, 1, D] (key/value), text_embeds: [B, T, D] (query)\n",
    "        attended, _ = self.attn(query=text_embeds, key=image_embed, value=image_embed)\n",
    "        return self.ln(text_embeds + attended)  # residual connection\n",
    "\n",
    "cross_attn = CrossAttention(phi_dim).to(device)\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = torch.optim.AdamW(list(projector.parameters()) + list(cross_attn.parameters()), lr=1e-4)\n",
    "\n",
    "# --- Loss ---\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=phi_tokenizer.pad_token_id)\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_projection_layer(dataloader, projector, epochs=3):\n",
    "    projector.train()\n",
    "    cross_attn.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            questions = batch[\"question\"]\n",
    "            answers = batch[\"answer\"]\n",
    "\n",
    "            # Visual features\n",
    "            with torch.no_grad():\n",
    "                vit_out = vit_model(pixel_values=images)\n",
    "                cls_token = vit_out.last_hidden_state[:, 0, :]  # [B, vit_dim]\n",
    "            projected_image = projector(cls_token).unsqueeze(1).to(torch.float16)  # [B, 1, phi_dim]\n",
    "\n",
    "            # Textual embeddings\n",
    "            inputs = phi_tokenizer(\n",
    "                [q + \" \" + phi_tokenizer.eos_token for q in questions],\n",
    "                return_tensors=\"pt\", padding=True, truncation=True\n",
    "            ).to(device)\n",
    "\n",
    "            labels = phi_tokenizer(\n",
    "                answers, return_tensors=\"pt\", padding=True, truncation=True\n",
    "            ).input_ids.to(device)\n",
    "\n",
    "            token_embeds = phi_model.model.embed_tokens(inputs.input_ids).to(torch.float16)  # [B, T, D]\n",
    "\n",
    "            # Cross-attention: text attends to image\n",
    "            fused_embeds = cross_attn(token_embeds, projected_image)  # [B, T, D]\n",
    "\n",
    "            attention_mask = inputs.attention_mask\n",
    "            labels = labels\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = phi_model(\n",
    "                inputs_embeds=fused_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code with some modification in the loss function calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code with some modification in the loss function calculation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import ViTModel, AutoConfig\n",
    "from safetensors.torch import load_file\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load ViT (Frozen) ---\n",
    "vit_config = AutoConfig.from_pretrained(\"./vit\")\n",
    "vit_model = ViTModel.from_pretrained(\"./vit\", config=vit_config)\n",
    "vit_model.load_state_dict(load_file(\"./vit/model.safetensors\"))\n",
    "vit_model = vit_model.to(device).eval()\n",
    "for p in vit_model.parameters(): p.requires_grad = False\n",
    "\n",
    "# --- Load Phi-2 (Frozen) ---\n",
    "phi_tokenizer = AutoTokenizer.from_pretrained(\"./phi-2\")\n",
    "phi_tokenizer.pad_token = phi_tokenizer.eos_token\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./phi-2\", torch_dtype=torch.float16, device_map={\"\": device}\n",
    ")\n",
    "phi_model.eval()\n",
    "for p in phi_model.parameters(): p.requires_grad = False\n",
    "\n",
    "# --- Dimensions ---\n",
    "vit_dim = vit_model.config.hidden_size\n",
    "phi_dim = phi_model.config.hidden_size\n",
    "\n",
    "# --- Projection Layer (Trainable) ---\n",
    "projector = nn.Linear(vit_dim, phi_dim).to(device)\n",
    "\n",
    "# --- Cross-Attention Module (Trainable) ---\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, text_embeds, image_embed):\n",
    "        # image_embed: [B, 1, D] (key/value), text_embeds: [B, T, D] (query)\n",
    "        attended, _ = self.attn(query=text_embeds, key=image_embed, value=image_embed)\n",
    "        return self.ln(text_embeds + attended)\n",
    "\n",
    "cross_attn = CrossAttention(phi_dim).to(device)\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = torch.optim.AdamW(list(projector.parameters()) + list(cross_attn.parameters()), lr=1e-4)\n",
    "\n",
    "# --- Explicit CrossEntropy Loss ---\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=phi_tokenizer.pad_token_id)\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_projection_layer(dataloader, projector, epochs=3):\n",
    "    projector.train()\n",
    "    cross_attn.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            questions = batch[\"question\"]\n",
    "            answers = batch[\"answer\"]\n",
    "\n",
    "            # ViT image features\n",
    "            with torch.no_grad():\n",
    "                vit_out = vit_model(pixel_values=images)\n",
    "                cls_token = vit_out.last_hidden_state[:, 0, :]\n",
    "            projected_image = projector(cls_token).unsqueeze(1).to(torch.float16)  # [B, 1, D]\n",
    "\n",
    "            # Tokenize prompt + answer\n",
    "            prompts = [f\"Question: {q.strip()} Answer:\" for q in questions]\n",
    "            full_texts = [f\"{p} {a.strip()}\" for p, a in zip(prompts, answers)]\n",
    "\n",
    "            inputs = phi_tokenizer(full_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            input_ids = inputs.input_ids.to(device)\n",
    "            attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "            # Generate label masks to ignore prompt\n",
    "            with phi_tokenizer.as_target_tokenizer():\n",
    "                prompt_ids = phi_tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "            labels = input_ids.clone()\n",
    "            for i in range(labels.size(0)):\n",
    "                prompt_len = (prompt_ids[i] != phi_tokenizer.pad_token_id).sum()\n",
    "                labels[i, :prompt_len] = -100\n",
    "\n",
    "            # Token embedding + cross-attention fusion\n",
    "            token_embeds = phi_model.get_input_embeddings()(input_ids).to(torch.float16)  # [B, T, D]\n",
    "            fused_embeds = cross_attn(token_embeds, projected_image)  # [B, T, D]\n",
    "\n",
    "            # Forward pass to get logits\n",
    "            logits = phi_model(inputs_embeds=fused_embeds, attention_mask=attention_mask).logits  # [B, T, V]\n",
    "\n",
    "            # Compute loss manually\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code with validation loop and ANLS metric\n",
    "1. Validation loop\n",
    "\n",
    "2. ANLS (Average Normalized Levenshtein Similarity) metric\n",
    "\n",
    "3. Manual CrossEntropyLoss\n",
    "\n",
    "4. Cross-attention fusion between ViT and Phi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Code with validation loop and ANLS metric\n",
    "import numpy as np\n",
    "import editdistance  # pip install editdistance\n",
    "\n",
    "def normalized_levenshtein(pred, gt):\n",
    "    pred, gt = pred.strip().lower(), gt.strip().lower()\n",
    "    if len(gt) == 0:\n",
    "        return 1.0 if len(pred) == 0 else 0.0\n",
    "    dist = editdistance.eval(pred, gt)\n",
    "    norm = dist / max(len(pred), len(gt))\n",
    "    return 1 - norm\n",
    "\n",
    "def compute_anls(preds, gts, threshold=0.5):\n",
    "    scores = []\n",
    "    for p, g in zip(preds, gts):\n",
    "        sim = normalized_levenshtein(p, g)\n",
    "        scores.append(sim if sim >= threshold else 0)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_projection_layer_with_validation(train_loader, val_loader, projector, epochs=3):\n",
    "    projector.train()\n",
    "    cross_attn.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_anls = []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}\"):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            questions = batch[\"question\"]\n",
    "            answers = batch[\"answer\"]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                vit_out = vit_model(pixel_values=images)\n",
    "                cls_token = vit_out.last_hidden_state[:, 0, :]\n",
    "            projected_image = projector(cls_token).unsqueeze(1).to(torch.float16)\n",
    "\n",
    "            prompts = [f\"Question: {q.strip()} Answer:\" for q in questions]\n",
    "            full_texts = [f\"{p} {a.strip()}\" for p, a in zip(prompts, answers)]\n",
    "\n",
    "            inputs = phi_tokenizer(full_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            input_ids = inputs.input_ids\n",
    "            attention_mask = inputs.attention_mask\n",
    "\n",
    "            with phi_tokenizer.as_target_tokenizer():\n",
    "                prompt_ids = phi_tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "            labels = input_ids.clone()\n",
    "            for i in range(labels.size(0)):\n",
    "                prompt_len = (prompt_ids[i] != phi_tokenizer.pad_token_id).sum()\n",
    "                labels[i, :prompt_len] = -100\n",
    "\n",
    "            token_embeds = phi_model.get_input_embeddings()(input_ids).to(torch.float16)\n",
    "            fused_embeds = cross_attn(token_embeds, projected_image)\n",
    "\n",
    "            logits = phi_model(inputs_embeds=fused_embeds, attention_mask=attention_mask).logits\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # --- In-batch ANLS ---\n",
    "            with torch.no_grad():\n",
    "                generated_ids = phi_model.generate(\n",
    "                    inputs_embeds=fused_embeds,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                decoded_preds = phi_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "                cleaned_preds = [pred.replace(prompt, \"\").strip() for pred, prompt in zip(decoded_preds, prompts)]\n",
    "                anls_score = compute_anls(cleaned_preds, answers)\n",
    "                total_anls.append(anls_score)\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_anls = np.mean(total_anls)\n",
    "        print(f\"[Train] Epoch {epoch+1} | Loss: {avg_loss:.4f} | ANLS: {avg_anls:.4f}\")\n",
    "\n",
    "        # Run Validation\n",
    "        run_validation(val_loader, projector)\n",
    "\n",
    "def run_validation(val_loader, projector):\n",
    "    projector.eval()\n",
    "    cross_attn.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"[Val]\"):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            questions = batch[\"question\"]\n",
    "            answers = batch[\"answer\"]\n",
    "\n",
    "            vit_out = vit_model(pixel_values=images)\n",
    "            cls_token = vit_out.last_hidden_state[:, 0, :]\n",
    "            projected_image = projector(cls_token).unsqueeze(1).to(torch.float16)\n",
    "\n",
    "            prompts = [f\"Question: {q.strip()} Answer:\" for q in questions]\n",
    "            full_texts = [f\"{p} {a.strip()}\" for p, a in zip(prompts, answers)]\n",
    "\n",
    "            inputs = phi_tokenizer(full_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            input_ids = inputs.input_ids\n",
    "            attention_mask = inputs.attention_mask\n",
    "\n",
    "            with phi_tokenizer.as_target_tokenizer():\n",
    "                prompt_ids = phi_tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "            labels = input_ids.clone()\n",
    "            for i in range(labels.size(0)):\n",
    "                prompt_len = (prompt_ids[i] != phi_tokenizer.pad_token_id).sum()\n",
    "                labels[i, :prompt_len] = -100\n",
    "\n",
    "            token_embeds = phi_model.get_input_embeddings()(input_ids).to(torch.float16)\n",
    "            fused_embeds = cross_attn(token_embeds, projected_image)\n",
    "\n",
    "            logits = phi_model(inputs_embeds=fused_embeds, attention_mask=attention_mask).logits\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            generated_ids = phi_model.generate(\n",
    "                inputs_embeds=fused_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=False\n",
    "            )\n",
    "            decoded_preds = phi_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            cleaned_preds = [pred.replace(prompt, \"\").strip() for pred, prompt in zip(decoded_preds, prompts)]\n",
    "            all_preds.extend(cleaned_preds)\n",
    "            all_labels.extend(answers)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    anls = compute_anls(all_preds, all_labels)\n",
    "    print(f\"[Val] Loss: {avg_loss:.4f} | ANLS: {anls:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_projection_layer_with_validation(\u001b[43mtrain_loader\u001b[49m, val_loader, projector, epochs=\u001b[32m3\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "train_projection_layer_with_validation(train_loader, val_loader, projector, epochs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including Model checkpoint saving on best ANLS and progress plots\n",
    "1. Model checkpointing based on the best validation ANLS\n",
    "\n",
    "2. Progress plots for training loss and validation ANLS\n",
    "\n",
    "3. Clean structure, preserving all previous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_projection_layer_with_validation_and_checkpoint(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    projector,\n",
    "    epochs=3,\n",
    "    checkpoint_dir=\"./checkpoints\"\n",
    "):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    projector.train()\n",
    "    cross_attn.train()\n",
    "\n",
    "    best_anls = -1\n",
    "    history = {\"train_loss\": [], \"val_anls\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_anls = []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}\"):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            questions = batch[\"question\"]\n",
    "            answers = batch[\"answer\"]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                vit_out = vit_model(pixel_values=images)\n",
    "                cls_token = vit_out.last_hidden_state[:, 0, :]\n",
    "            projected_image = projector(cls_token).unsqueeze(1).to(torch.float16)\n",
    "\n",
    "            prompts = [f\"Question: {q.strip()} Answer:\" for q in questions]\n",
    "            full_texts = [f\"{p} {a.strip()}\" for p, a in zip(prompts, answers)]\n",
    "\n",
    "            inputs = phi_tokenizer(full_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            input_ids = inputs.input_ids\n",
    "            attention_mask = inputs.attention_mask\n",
    "\n",
    "            with phi_tokenizer.as_target_tokenizer():\n",
    "                prompt_ids = phi_tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "            labels = input_ids.clone()\n",
    "            for i in range(labels.size(0)):\n",
    "                prompt_len = (prompt_ids[i] != phi_tokenizer.pad_token_id).sum()\n",
    "                labels[i, :prompt_len] = -100\n",
    "\n",
    "            token_embeds = phi_model.get_input_embeddings()(input_ids).to(torch.float16)\n",
    "            fused_embeds = cross_attn(token_embeds, projected_image)\n",
    "\n",
    "            logits = phi_model(inputs_embeds=fused_embeds, attention_mask=attention_mask).logits\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                generated_ids = phi_model.generate(\n",
    "                    inputs_embeds=fused_embeds,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                decoded_preds = phi_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "                cleaned_preds = [pred.replace(prompt, \"\").strip() for pred, prompt in zip(decoded_preds, prompts)]\n",
    "                anls_score = compute_anls(cleaned_preds, answers)\n",
    "                total_anls.append(anls_score)\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_anls = run_validation(val_loader, projector, save_best=True, best_anls=best_anls, checkpoint_dir=checkpoint_dir)\n",
    "\n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "        history[\"val_anls\"].append(avg_anls)\n",
    "\n",
    "        print(f\"[Train] Epoch {epoch+1} | Loss: {avg_loss:.4f} | Train ANLS: {np.mean(total_anls):.4f} | Val ANLS: {avg_anls:.4f}\")\n",
    "\n",
    "        if avg_anls > best_anls:\n",
    "            best_anls = avg_anls\n",
    "            print(f\"✅ New best ANLS: {best_anls:.4f} — model saved.\")\n",
    "\n",
    "    # Plot loss and ANLS\n",
    "    plot_metrics(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(val_loader, projector, save_best=False, best_anls=-1, checkpoint_dir=\"./checkpoints\"):\n",
    "    projector.eval()\n",
    "    cross_attn.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"[Val]\"):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            questions = batch[\"question\"]\n",
    "            answers = batch[\"answer\"]\n",
    "\n",
    "            vit_out = vit_model(pixel_values=images)\n",
    "            cls_token = vit_out.last_hidden_state[:, 0, :]\n",
    "            projected_image = projector(cls_token).unsqueeze(1).to(torch.float16)\n",
    "\n",
    "            prompts = [f\"Question: {q.strip()} Answer:\" for q in questions]\n",
    "            full_texts = [f\"{p} {a.strip()}\" for p, a in zip(prompts, answers)]\n",
    "\n",
    "            inputs = phi_tokenizer(full_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            input_ids = inputs.input_ids\n",
    "            attention_mask = inputs.attention_mask\n",
    "\n",
    "            with phi_tokenizer.as_target_tokenizer():\n",
    "                prompt_ids = phi_tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "            labels = input_ids.clone()\n",
    "            for i in range(labels.size(0)):\n",
    "                prompt_len = (prompt_ids[i] != phi_tokenizer.pad_token_id).sum()\n",
    "                labels[i, :prompt_len] = -100\n",
    "\n",
    "            token_embeds = phi_model.get_input_embeddings()(input_ids).to(torch.float16)\n",
    "            fused_embeds = cross_attn(token_embeds, projected_image)\n",
    "\n",
    "            logits = phi_model(inputs_embeds=fused_embeds, attention_mask=attention_mask).logits\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            generated_ids = phi_model.generate(\n",
    "                inputs_embeds=fused_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=False\n",
    "            )\n",
    "            decoded_preds = phi_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            cleaned_preds = [pred.replace(prompt, \"\").strip() for pred, prompt in zip(decoded_preds, prompts)]\n",
    "            all_preds.extend(cleaned_preds)\n",
    "            all_labels.extend(answers)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    anls = compute_anls(all_preds, all_labels)\n",
    "\n",
    "    # Save checkpoint if best\n",
    "    if save_best and anls > best_anls:\n",
    "        from safetensors.torch import save_file\n",
    "        save_file(projector.state_dict(), f\"{checkpoint_dir}/projector_best_anls.safetensors\")\n",
    "        save_file(cross_attn.state_dict(), f\"{checkpoint_dir}/cross_attn_best_anls.safetensors\")\n",
    "\n",
    "    print(f\"[Val] Loss: {avg_loss:.4f} | ANLS: {anls:.4f}\")\n",
    "    return anls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plotting function\n",
    "\n",
    "def plot_metrics(history):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history[\"val_anls\"], label=\"Validation ANLS\", color=\"green\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"ANLS\")\n",
    "    plt.title(\"Validation ANLS\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To run the model\n",
    "train_projection_layer_with_validation_and_checkpoint(train_loader, val_loader, projector, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resume training from a checkpoint and save model in .safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####    Add helper functions to save and load checkpoint:\n",
    "\n",
    "from safetensors.torch import save_file, load_file\n",
    "\n",
    "def save_checkpoint(projector, cross_attn, checkpoint_dir=\"./checkpoints\"):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    save_file(projector.state_dict(), f\"{checkpoint_dir}/projector_best_anls.safetensors\")\n",
    "    save_file(cross_attn.state_dict(), f\"{checkpoint_dir}/cross_attn_best_anls.safetensors\")\n",
    "    print(f\"✅ Checkpoint saved to {checkpoint_dir}\")\n",
    "\n",
    "def load_checkpoint(projector, cross_attn, checkpoint_dir=\"./checkpoints\"):\n",
    "    projector.load_state_dict(load_file(f\"{checkpoint_dir}/projector_best_anls.safetensors\"))\n",
    "    cross_attn.load_state_dict(load_file(f\"{checkpoint_dir}/cross_attn_best_anls.safetensors\"))\n",
    "    print(f\"🔁 Resumed from checkpoint in {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   Add resume Option to Training Function......Update training function definition like:\n",
    "\n",
    "def train_projection_layer_with_validation_and_checkpoint(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    projector,\n",
    "    epochs=3,\n",
    "    checkpoint_dir=\"./checkpoints\",\n",
    "    resume=False\n",
    "):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    if resume:\n",
    "        load_checkpoint(projector, cross_attn, checkpoint_dir)\n",
    "\n",
    "    projector.train()\n",
    "    cross_attn.train()\n",
    "    best_anls = -1\n",
    "    history = {\"train_loss\": [], \"val_anls\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######   Inside validation:\n",
    "\n",
    "# Save if best\n",
    "if save_best and anls > best_anls:\n",
    "    best_anls = anls\n",
    "    save_checkpoint(projector, cross_attn, checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   Run training with Resume\n",
    "\n",
    "train_projection_layer_with_validation_and_checkpoint(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    projector,\n",
    "    epochs=5,\n",
    "    checkpoint_dir=\"./checkpoints\",\n",
    "    resume=True  # <-- Resume from .safetensors\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project1_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
