{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to develop a vision language model using ViT as vision encoder and Phi-2 as LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from transformers import ViTModel\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# ---- Configuration ----\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "image_path = \"image.jpg\"\n",
    "vit_model_path = \"./vit\"\n",
    "phi2_model_path = \"./phi-2\"\n",
    "prompt = \"Describe this image in detail.\"\n",
    "\n",
    "# ---- 1. Load ViT Encoder (from safetensors) ----\n",
    "vit_config = AutoConfig.from_pretrained(vit_model_path)\n",
    "vit_model = ViTModel.from_pretrained(vit_model_path, config=vit_config)\n",
    "vit_model.load_state_dict(load_file(f\"{vit_model_path}/model.safetensors\"))\n",
    "vit_model = vit_model.to(device).eval()\n",
    "\n",
    "# ---- 2. Load Phi-2 from Local Directory ----\n",
    "phi_tokenizer = AutoTokenizer.from_pretrained(phi2_model_path)\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "    phi2_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# ---- 3. Vision-Language Wrapper ----\n",
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(self, vit, phi2, vit_dim, phi_dim):\n",
    "        super().__init__()\n",
    "        self.vit = vit\n",
    "        self.phi2 = phi2\n",
    "        self.projector = nn.Linear(vit_dim, phi_dim)\n",
    "\n",
    "    def forward(self, image_tensor, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            vision_out = self.vit(pixel_values=image_tensor)\n",
    "            cls_token = vision_out.last_hidden_state[:, 0, :]  # shape: [B, vit_dim]\n",
    "\n",
    "        vision_embed = self.projector(cls_token).unsqueeze(1)  # [B, 1, phi_dim]\n",
    "        token_embeds = self.phi2.transformer.wte(input_ids)\n",
    "        full_embeds = torch.cat([vision_embed, token_embeds], dim=1)\n",
    "\n",
    "        attention_mask = torch.cat(\n",
    "            [torch.ones((attention_mask.shape[0], 1), dtype=attention_mask.dtype).to(device), attention_mask],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        out = self.phi2(inputs_embeds=full_embeds, attention_mask=attention_mask, use_cache=True)\n",
    "        return out\n",
    "\n",
    "# ---- 4. Preprocess Image ----\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "# ---- 5. Prepare Text Inputs ----\n",
    "input_ids = phi_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "# ---- 6. Build Wrapper and Run Inference ----\n",
    "vit_dim = vit_model.config.hidden_size\n",
    "phi_dim = phi_model.config.hidden_size  # Phi-2 hidden size is 2048\n",
    "\n",
    "wrapper = VisionLanguageModel(vit_model, phi_model, vit_dim, phi_dim).to(device)\n",
    "\n",
    "# ---- 7. Generate Tokens ----\n",
    "with torch.no_grad():\n",
    "    out = wrapper(image_tensor, input_ids, attention_mask)\n",
    "    next_token = torch.argmax(out.logits[:, -1:, :], dim=-1)\n",
    "    for _ in range(50):  # change to desired max tokens\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        out = wrapper(image_tensor, input_ids, attention_mask)\n",
    "        next_token = torch.argmax(out.logits[:, -1:, :], dim=-1)\n",
    "\n",
    "# ---- 8. Decode and Print Result ----\n",
    "caption = phi_tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "print(\"Image Summary:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Giving error 'PhiForCausalLM' object has no attribute 'transformer' in the vision language wrapper section forward function. Just change the error part and leave as it is\n",
    "#Chnage below line to \n",
    "token_embeds = self.phi2.transformer.wte(input_ids)\n",
    "#To\n",
    "token_embeds = self.phi2.model.embed_tokens(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Training  code\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import ViTModel, AutoConfig\n",
    "from safetensors.torch import load_file\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load ViT (Frozen) ---\n",
    "vit_config = AutoConfig.from_pretrained(\"./vit\")\n",
    "vit_model = ViTModel.from_pretrained(\"./vit\", config=vit_config)\n",
    "vit_model.load_state_dict(load_file(\"./vit/model.safetensors\"))\n",
    "vit_model = vit_model.to(device).eval()\n",
    "for p in vit_model.parameters(): p.requires_grad = False\n",
    "\n",
    "# --- Load Phi-2 (Frozen) ---\n",
    "phi_tokenizer = AutoTokenizer.from_pretrained(\"./phi-2\")\n",
    "phi_tokenizer.pad_token = phi_tokenizer.eos_token\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./phi-2\", torch_dtype=torch.float16, device_map={\"\": device}\n",
    ")\n",
    "phi_model.eval()\n",
    "for p in phi_model.parameters(): p.requires_grad = False\n",
    "\n",
    "# --- Projection Layer (Trainable) ---\n",
    "vit_dim = vit_model.config.hidden_size\n",
    "phi_dim = phi_model.config.hidden_size\n",
    "projector = nn.Linear(vit_dim, phi_dim).to(device)\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = torch.optim.AdamW(projector.parameters(), lr=1e-4)\n",
    "\n",
    "# --- Loss ---\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=phi_tokenizer.pad_token_id)\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_projection_layer(dataloader, projector,epochs=3):\n",
    "    projector.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            images = batch[\"image\"].to(device)  # shape: [B, 3, H, W]\n",
    "            questions = batch[\"question\"]\n",
    "            answers = batch[\"answer\"]\n",
    "\n",
    "            # Prepare image embeddings\n",
    "            with torch.no_grad():\n",
    "                vit_out = vit_model(pixel_values=images)\n",
    "                cls_token = vit_out.last_hidden_state[:, 0, :]\n",
    "            projected_embed = projector(cls_token).unsqueeze(1).to(torch.float16)\n",
    "\n",
    "            # Tokenize prompt + answer\n",
    "            inputs = phi_tokenizer([q + \" \" + phi_tokenizer.eos_token for q in questions],\n",
    "                                   return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            labels = phi_tokenizer(answers, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "            token_embeds = phi_model.model.embed_tokens(inputs.input_ids).to(torch.float16)\n",
    "            inputs_embeds = torch.cat([projected_embed, token_embeds], dim=1)\n",
    "\n",
    "            # Adjust attention and labels\n",
    "            attention_mask = torch.cat([\n",
    "                torch.ones((inputs.input_ids.shape[0], 1), dtype=torch.long).to(device),\n",
    "                inputs.attention_mask\n",
    "            ], dim=1)\n",
    "\n",
    "            labels = torch.cat([\n",
    "                torch.full((labels.shape[0], 1), fill_value=-100).to(device),\n",
    "                labels\n",
    "            ], dim=1)\n",
    "\n",
    "            # Forward pass\n",
    "            output = phi_model(inputs_embeds=inputs_embeds,\n",
    "                               attention_mask=attention_mask,\n",
    "                               labels=labels)\n",
    "\n",
    "            loss = output.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######   Function with mask input before Answer\n",
    "\n",
    "def train_projection_layer(dataloader, projector, epochs=3):\n",
    "    projector.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            images = batch[\"image\"].to(device)  # [B, 3, H, W]\n",
    "            questions = batch[\"question\"]\n",
    "            answers = batch[\"answer\"]\n",
    "\n",
    "            # ---- ViT: image to embedding ----\n",
    "            with torch.no_grad():\n",
    "                vit_out = vit_model(pixel_values=images)\n",
    "                cls_token = vit_out.last_hidden_state[:, 0, :]\n",
    "            projected_embed = projector(cls_token).unsqueeze(1).to(torch.float16)  # [B, 1, D]\n",
    "\n",
    "            # ---- Construct full prompt ----\n",
    "            prompts = [f\"Question: {q.strip()} Answer:\" for q in questions]\n",
    "            full_texts = [f\"{prompt} {ans.strip()}\" for prompt, ans in zip(prompts, answers)]\n",
    "\n",
    "            # ---- Tokenize full prompt ----\n",
    "            inputs = phi_tokenizer(full_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            input_ids = inputs.input_ids.to(device)\n",
    "            attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "            # ---- Tokenize prompt alone to get masking index ----\n",
    "            with phi_tokenizer.as_target_tokenizer():\n",
    "                prompt_ids = phi_tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "            # ---- Build label tensor ----\n",
    "            labels = input_ids.clone()\n",
    "            for i in range(labels.shape[0]):\n",
    "                prompt_len = (prompt_ids[i] != phi_tokenizer.pad_token_id).sum()\n",
    "                labels[i, :prompt_len] = -100  # mask prompt tokens from loss\n",
    "\n",
    "            # ---- Embed text ----\n",
    "            token_embeds = phi_model.model.embed_tokens(input_ids).to(torch.float16)  # [B, T, D]\n",
    "\n",
    "            # ---- Concatenate <image> + tokens ----\n",
    "            inputs_embeds = torch.cat([projected_embed, token_embeds], dim=1)\n",
    "            attention_mask = torch.cat([\n",
    "                torch.ones((attention_mask.shape[0], 1), dtype=torch.long).to(device),\n",
    "                attention_mask\n",
    "            ], dim=1)\n",
    "            labels = torch.cat([\n",
    "                torch.full((labels.shape[0], 1), fill_value=-100).to(device),  # mask image token\n",
    "                labels\n",
    "            ], dim=1)\n",
    "\n",
    "            # ---- Forward ----\n",
    "            outputs = phi_model(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Updated code Using ANLS metric* \n",
    "1. Trains only the projection layer.\n",
    "2. Formats input as \"<image> Question: ... Answer: ...\" and masks prompt + image during training loss computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (654833717.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mUpdated code Using ANLS metric\u001b[39m\n            ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import editdistance  # Install via: pip install editdistance\n",
    "\n",
    "def normalized_levenshtein(pred, gt):\n",
    "    \"\"\"Compute Normalized Levenshtein Similarity\"\"\"\n",
    "    pred, gt = pred.strip().lower(), gt.strip().lower()\n",
    "    if len(gt) == 0:\n",
    "        return 1.0 if len(pred) == 0 else 0.0\n",
    "    dist = editdistance.eval(pred, gt)\n",
    "    norm = dist / max(len(pred), len(gt))\n",
    "    return 1 - norm\n",
    "\n",
    "def compute_anls(preds, gts, threshold=0.5):\n",
    "    \"\"\"Average Normalized Levenshtein Similarity (ANLS)\"\"\"\n",
    "    scores = []\n",
    "    for p, g in zip(preds, gts):\n",
    "        sim = normalized_levenshtein(p, g)\n",
    "        scores.append(sim if sim >= threshold else 0)\n",
    "    return np.mean(scores)\n",
    "\n",
    "def train_projection_layer(dataloader, projector, epochs=3):\n",
    "    projector.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_anls = []\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            questions = batch[\"question\"]\n",
    "            answers = batch[\"answer\"]\n",
    "\n",
    "            # --- Image Embedding ---\n",
    "            with torch.no_grad():\n",
    "                vit_out = vit_model(pixel_values=images)\n",
    "                cls_token = vit_out.last_hidden_state[:, 0, :]\n",
    "            projected_embed = projector(cls_token).unsqueeze(1).to(torch.float16)\n",
    "\n",
    "            # --- Prompt Construction ---\n",
    "            prompts = [f\"Question: {q.strip()} Answer:\" for q in questions]\n",
    "            full_texts = [f\"{p} {a.strip()}\" for p, a in zip(prompts, answers)]\n",
    "\n",
    "            inputs = phi_tokenizer(full_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            input_ids = inputs.input_ids.to(device)\n",
    "            attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "            with phi_tokenizer.as_target_tokenizer():\n",
    "                prompt_ids = phi_tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "            # --- Labels Masking ---\n",
    "            labels = input_ids.clone()\n",
    "            for i in range(labels.shape[0]):\n",
    "                prompt_len = (prompt_ids[i] != phi_tokenizer.pad_token_id).sum()\n",
    "                labels[i, :prompt_len] = -100\n",
    "\n",
    "            # --- Embedding + Concatenation ---\n",
    "            token_embeds = phi_model.model.embed_tokens(input_ids).to(torch.float16)\n",
    "            inputs_embeds = torch.cat([projected_embed, token_embeds], dim=1)\n",
    "\n",
    "            attention_mask = torch.cat([\n",
    "                torch.ones((attention_mask.shape[0], 1), dtype=torch.long).to(device),\n",
    "                attention_mask\n",
    "            ], dim=1)\n",
    "            labels = torch.cat([\n",
    "                torch.full((labels.shape[0], 1), fill_value=-100).to(device),\n",
    "                labels\n",
    "            ], dim=1)\n",
    "\n",
    "            # --- Forward ---\n",
    "            outputs = phi_model(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # --- Decode and Evaluate ANLS ---\n",
    "            with torch.no_grad():\n",
    "                generated_ids = phi_model.generate(\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                decoded_preds = phi_tokenizer.batch_decode(generated_ids[:, 1:], skip_special_tokens=True)\n",
    "                decoded_preds = [pred.replace(prompt, \"\").strip() for pred, prompt in zip(decoded_preds, prompts)]\n",
    "\n",
    "                anls_score = compute_anls(decoded_preds, answers)\n",
    "                total_anls.append(anls_score)\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_anls = np.mean(total_anls)\n",
    "        print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | ANLS: {avg_anls:.4f}\")\n",
    "\n",
    "\n",
    "######  Output format:   Epoch 1 | Loss: 1.2345 | ANLS: 0.8421\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the inference code that uses the trained projector and computes the Average Normalized Levenshtein Similarity (ANLS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'editdistance'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01meditdistance\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnormalized_levenshtein\u001b[39m(pred, gt):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'editdistance'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import editdistance\n",
    "from tqdm import tqdm\n",
    "\n",
    "def normalized_levenshtein(pred, gt):\n",
    "    pred, gt = pred.strip().lower(), gt.strip().lower()\n",
    "    if len(gt) == 0:\n",
    "        return 1.0 if len(pred) == 0 else 0.0\n",
    "    dist = editdistance.eval(pred, gt)\n",
    "    norm = dist / max(len(pred), len(gt))\n",
    "    return 1 - norm\n",
    "\n",
    "def compute_anls(preds, gts, threshold=0.5):\n",
    "    scores = []\n",
    "    for p, g in zip(preds, gts):\n",
    "        sim = normalized_levenshtein(p, g)\n",
    "        scores.append(sim if sim >= threshold else 0)\n",
    "    return np.mean(scores)\n",
    "\n",
    "def run_inference(dataloader, vit_model, projector, phi_model, phi_tokenizer):\n",
    "    vit_model.eval()\n",
    "    projector.eval()\n",
    "    phi_model.eval()\n",
    "\n",
    "    all_preds, all_gts, all_prompts = [], [], []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Inference\"):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        questions = batch[\"question\"]\n",
    "        answers = batch[\"answer\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Image embedding\n",
    "            vit_out = vit_model(pixel_values=images)\n",
    "            cls_token = vit_out.last_hidden_state[:, 0, :]\n",
    "            projected_embed = projector(cls_token).unsqueeze(1).to(torch.float16)\n",
    "\n",
    "            # Prompt: \"<image> Question: ... Answer:\"\n",
    "            prompts = [f\"Question: {q.strip()} Answer:\" for q in questions]\n",
    "            inputs = phi_tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            input_ids = inputs.input_ids.to(device)\n",
    "            attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "            token_embeds = phi_model.model.embed_tokens(input_ids).to(torch.float16)\n",
    "            inputs_embeds = torch.cat([projected_embed, token_embeds], dim=1)\n",
    "            attention_mask = torch.cat([\n",
    "                torch.ones((attention_mask.shape[0], 1), dtype=torch.long).to(device),\n",
    "                attention_mask\n",
    "            ], dim=1)\n",
    "\n",
    "            # Generate answer\n",
    "            generated_ids = phi_model.generate(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=False\n",
    "            )\n",
    "            decoded_preds = phi_tokenizer.batch_decode(generated_ids[:, 1:], skip_special_tokens=True)\n",
    "            cleaned_preds = [pred.replace(prompt, \"\").strip() for pred, prompt in zip(decoded_preds, prompts)]\n",
    "\n",
    "            all_preds.extend(cleaned_preds)\n",
    "            all_gts.extend(answers)\n",
    "            all_prompts.extend(prompts)\n",
    "\n",
    "    # ANLS Score\n",
    "    anls_score = compute_anls(all_preds, all_gts)\n",
    "    print(f\"\\nAverage ANLS: {anls_score:.4f}\")\n",
    "    return all_preds, all_gts, all_prompts\n",
    "###  Requirements;\n",
    "###   batch[\"image\"], batch[\"question\"], batch[\"answer\"] must be provided by the dataloader.\n",
    "###### projector, vit_model, phi_model, and phi_tokenizer must be loaded and moved to correct device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project1_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
