{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python code to train the projection layer with another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from safetensors.torch import load_file\n",
    "from transformers import ViTModel, AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load ViT Model (Frozen) ---\n",
    "vit_config = AutoConfig.from_pretrained(\"./vit\")\n",
    "vit_model = ViTModel.from_pretrained(\"./vit\", config=vit_config)\n",
    "vit_model.load_state_dict(load_file(\"./vit/model.safetensors\"))\n",
    "vit_model.to(device).eval()\n",
    "for p in vit_model.parameters(): p.requires_grad = False\n",
    "\n",
    "# --- Load Phi-2 (Frozen) ---\n",
    "phi_tokenizer = AutoTokenizer.from_pretrained(\"./phi-2\")\n",
    "phi_tokenizer.pad_token = phi_tokenizer.eos_token\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(\"./phi-2\", torch_dtype=torch.float16, device_map={\"\": device})\n",
    "phi_model.eval()\n",
    "for p in phi_model.parameters(): p.requires_grad = False\n",
    "\n",
    "# --- Load Trained Projection Layer ---\n",
    "vit_dim = vit_model.config.hidden_size\n",
    "phi_dim = phi_model.config.hidden_size\n",
    "projector = nn.Linear(vit_dim, phi_dim).to(device)\n",
    "projector.load_state_dict(load_file(\"projector_finetuned.safetensors\"))\n",
    "projector.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_projection_layer(dataloader, projector, num_epochs=3, lr=1e-4):\n",
    "    optimizer = torch.optim.AdamW(projector.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=phi_tokenizer.pad_token_id)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        projector.train()\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            questions = batch[\"question\"]\n",
    "            answers = batch[\"answer\"]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                vit_out = vit_model(pixel_values=images)\n",
    "                cls_token = vit_out.last_hidden_state[:, 0, :]\n",
    "            projected_embed = projector(cls_token).unsqueeze(1).to(torch.float16)\n",
    "\n",
    "            prompts = [f\"Question: {q.strip()} Answer:\" for q in questions]\n",
    "            full_texts = [f\"{p} {a.strip()}\" for p, a in zip(prompts, answers)]\n",
    "\n",
    "            inputs = phi_tokenizer(full_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            input_ids = inputs.input_ids.to(device)\n",
    "            attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "            with phi_tokenizer.as_target_tokenizer():\n",
    "                prompt_ids = phi_tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "            labels = input_ids.clone()\n",
    "            for i in range(labels.shape[0]):\n",
    "                prompt_len = (prompt_ids[i] != phi_tokenizer.pad_token_id).sum()\n",
    "                labels[i, :prompt_len] = -100\n",
    "\n",
    "            token_embeds = phi_model.model.embed_tokens(input_ids).to(torch.float16)\n",
    "            inputs_embeds = torch.cat([projected_embed, token_embeds], dim=1)\n",
    "\n",
    "            attention_mask = torch.cat([\n",
    "                torch.ones((attention_mask.shape[0], 1), dtype=torch.long).to(device),\n",
    "                attention_mask\n",
    "            ], dim=1)\n",
    "            labels = torch.cat([\n",
    "                torch.full((labels.shape[0], 1), fill_value=-100).to(device),\n",
    "                labels\n",
    "            ], dim=1)\n",
    "\n",
    "            output = phi_model(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = output.loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Loss: {epoch_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Load the complete model for the Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, ViTModel\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load ViT (Frozen) ---\n",
    "vit_config = AutoConfig.from_pretrained(\"./vit\")\n",
    "vit_model = ViTModel.from_pretrained(\"./vit\", config=vit_config)\n",
    "vit_model.load_state_dict(load_file(\"./vit/model.safetensors\"))\n",
    "vit_model = vit_model.to(device).eval()\n",
    "for p in vit_model.parameters(): p.requires_grad = False\n",
    "\n",
    "# --- Load Phi-2 Model (Frozen) ---\n",
    "phi_tokenizer = AutoTokenizer.from_pretrained(\"./phi-2\")\n",
    "phi_tokenizer.pad_token = phi_tokenizer.eos_token  # ensure pad_token exists\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./phi-2\", torch_dtype=torch.float16, device_map={\"\": device}\n",
    ")\n",
    "phi_model.eval()\n",
    "for p in phi_model.parameters(): p.requires_grad = False\n",
    "\n",
    "# --- Load Trained Projection Layer ---\n",
    "vit_dim = vit_model.config.hidden_size\n",
    "phi_dim = phi_model.config.hidden_size\n",
    "\n",
    "projector = nn.Linear(vit_dim, phi_dim).to(device)\n",
    "projector.load_state_dict(load_file(\"projector_finetuned.safetensors\"))\n",
    "projector.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(image_tensor, question_text):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image_tensor: torch.Tensor of shape [1, 3, H, W]\n",
    "        question_text: str\n",
    "    Returns:\n",
    "        generated answer: str\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # 1. Get ViT image embedding\n",
    "        vit_out = vit_model(pixel_values=image_tensor.to(device))\n",
    "        cls_token = vit_out.last_hidden_state[:, 0, :]  # [1, vit_dim]\n",
    "        projected_embed = projector(cls_token).unsqueeze(1).to(torch.float16)  # [1, 1, phi_dim]\n",
    "\n",
    "        # 2. Tokenize prompt\n",
    "        prompt = f\"Question: {question_text.strip()} Answer:\"\n",
    "        inputs = phi_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        token_embeds = phi_model.get_input_embeddings()(inputs.input_ids).to(torch.float16)\n",
    "        inputs_embeds = torch.cat([projected_embed, token_embeds], dim=1)\n",
    "\n",
    "        attention_mask = torch.cat([\n",
    "            torch.ones((1, 1), dtype=torch.long).to(device),\n",
    "            inputs.attention_mask\n",
    "        ], dim=1)\n",
    "\n",
    "        # 3. Generate answer\n",
    "        generated_ids = phi_model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=False\n",
    "        )\n",
    "        output_text = phi_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Remove the prompt from generated text\n",
    "        return output_text.replace(prompt, \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # match ViT input size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "image = Image.open(\"test_image.png\").convert(\"RGB\")\n",
    "image_tensor = transform(image).unsqueeze(0)  # shape: [1, 3, H, W]\n",
    "\n",
    "# Inference\n",
    "question = \"What is the name of the document?\"\n",
    "answer = run_inference(image_tensor, question)\n",
    "print(\"Generated Answer:\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project1_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
